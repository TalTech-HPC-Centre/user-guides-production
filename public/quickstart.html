<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quickstart: Cluster &mdash; HPC user-guides 2024 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/extra.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Courses and introductions" href="learning.html" />
    <link rel="prev" title="Quickstart: Cloud" href="cloud.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #E4067E" >

          
          
          <a href="index.html" class="icon icon-home">
            HPC user-guides
              <img src="_static/TalTech_Gradient-200px.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lumi.html">LUMI</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud.html">Quickstart: Cloud</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart: Cluster</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#accessing-the-cluster">Accessing the cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#structure-and-file-tree">Structure and file tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-jobs-with-slurm">Running jobs with SLURM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slurm-accounts">SLURM accounts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring-jobs-resources">Monitoring jobs &amp; resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-a-job-on-the-node">Monitoring a job on the node</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#status-of-a-job">Status of a job</a></li>
<li class="toctree-l4"><a class="reference internal" href="#load-of-the-node">Load of the node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-with-interactive-job">Monitoring with interactive job</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-resource-usage">Monitoring resource usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#copying-data-to-from-the-clusters">Copying data to/from the clusters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#smb-cifs-exported-filesystems">SMB/CIFS exported filesystems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#windows-access">Windows access</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linux-access">Linux access</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#special-considerations-for-copying-windows-linux">Special considerations for copying Windows - Linux</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#backup">Backup</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="learning.html">Courses and introductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Module environment (lmod)</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpi.html">Available MPI versions (and comparison)</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiling.html">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPU-servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity.html">Containers (Singularity &amp; Docker)</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgement.html">Acknowledgement</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #E4067E" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">HPC user-guides</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quickstart: Cluster</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/quickstart.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quickstart-cluster">
<h1>Quickstart: Cluster<a class="headerlink" href="#quickstart-cluster" title="Permalink to this heading"></a></h1>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr><section id="accessing-the-cluster">
<h2>Accessing the cluster<a class="headerlink" href="#accessing-the-cluster" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p><strong>NB! To access the cluster, user must have an active <a class="reference external" href="https://taltech.atlassian.net/wiki/spaces/ITI/pages/38994346/Uni-ID+ehk+Digitaalne+identiteet">Uni-ID account</a>.</strong> For people who are neither students nor employees of Taltech <a class="reference external" href="https://taltech.atlassian.net/wiki/spaces/ITI/pages/38994346/Uni-ID+ehk+Digitaalne+identiteet#External-UNI-ID">Uni-ID non-contractual account</a> should be created by the head of a structural unit.</p>
<p><strong>To get access to HPC contact us by email (hpcsupport&#64;taltech.ee) or <a class="reference external" href="https://taltech.atlassian.net/servicedesk/customer/portal/18">Taltech portal</a>.</strong> We need the following information: uni-ID, department, project that covers <a class="reference external" href="/index.html#billing">costs</a>.</p>
<p><strong>The cluster is accessible from inside the university</strong> and from major <strong>Estonian network</strong> providers. If you are traveling (or not in one of the major networks), the access requires <a class="reference external" href="https://taltech.atlassian.net/wiki/spaces/ITI/pages/38994267/Kaug+hendus+FortiClient+VPN+Remote+connection+with+FortiClient+VPN">FortiVPN</a> (for OnDemand session and <code class="docutils literal notranslate"><span class="pre">ssh</span></code> command).</p>
<p>To access the cluster <strong>base.hpc.taltech.ee</strong> via a browser with a graphical, menu-based environment use desktop session on <a class="reference external" href="https://ondemand.hpc.taltech.ee">https://ondemand.hpc.taltech.ee</a></p>
<div style="width:85%; height:!85%; margin-left: auto; margin-right: auto;"> <p><img alt="ondemand" src="_images/ondemand.png" /></p>
</div><p><a class="reference internal" href="ondemand.html"><span class="doc">more about OnDemand sessions</span></a>.</p>
<p>Another option is SSH (the Secure SHell), available by command <code class="docutils literal notranslate"><span class="pre">ssh</span></code> in <strong>Linux/Unix, Mac</strong> and <strong>Windows-10.</strong>  A PuTTY guide for Windows users (an alternative SSH using a graphical user interface (GUI)) is <a class="reference internal" href="putty.html"><span class="doc">here</span></a>.</p>
<p>For using graphical applications add the <code class="docutils literal notranslate"><span class="pre">-X</span></code>, and for GLX (X Window System) forwarding additionally the <code class="docutils literal notranslate"><span class="pre">-Y</span></code> switch,:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>-X<span class="w"> </span>-Y<span class="w"> </span>uni-ID@base.hpc.taltech.ee
</pre></div>
</div>
<p><em><strong>where uni-ID should be changed to user’s uni-ID.</strong></em></p>
<p><em><strong>NB!</strong></em> <strong>The login-node is for some light interactive analysis.</strong> For heavy computations, request a (interactive) session with the resource manager <a class="reference external" href="/quickstart.html#running-jobs-with-slurm">SLURM</a> or submit job for execution by SLURM sbatch script!**</p>
<p>We strongly recommend to <strong>use SSH-keys for logging</strong> to the cluster with ssh command. <a class="reference internal" href="ssh.html"><span class="doc">How to get SSH keys</span></a>.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="structure-and-file-tree">
<h2>Structure and file tree<a class="headerlink" href="#structure-and-file-tree" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>By accessing the cluster, the user gets into home directory or <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> (<code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/home/$USER/</span></code>).</p>
<p>In the home directory, the user can create, delete, and overwrite files and perform calculations (if slurm script does not force program to use <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory). The home directory is limited in size of 500 GB and backups are performed once per week.</p>
<p>The home directory can be accessed from console or by GUI programs, but it cannot be mounted. For mounting was created special <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> and <code class="docutils literal notranslate"><span class="pre">smbgroup</span></code> folders (<code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/smbhome/$USER/</span></code> and <code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/smbgroup/</span></code>, respectively). More about <code class="docutils literal notranslate"><span class="pre">smb</span></code> folders can be found <a class="reference external" href="https://docs.hpc.taltech.ee/quickstart.html#smb-cifs-exported-filesystems">here</a>.</p>
<p>Some programs and scripts suppose that files will be transfer to <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory at compute node and calculations will be done there. If job will be killed, for example due  to the time limit back transfer will not occur. In this case, user needs to know at which node this job was running (see <code class="docutils literal notranslate"><span class="pre">slurm-$job_id.stat</span></code>), to connect to exactly this node (in example it is green11). <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory will be in <code class="docutils literal notranslate"><span class="pre">/state/partition1/</span></code> and corresponds to jobID number.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>-w<span class="w"> </span>green11<span class="w"> </span>--pty<span class="w"> </span>bash
<span class="nb">cd</span><span class="w"> </span>/state/partition1/
</pre></div>
</div>
<p>Please note that the scratch is <em>not</em> shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other’s scratch files.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="running-jobs-with-slurm">
<h2>Running jobs with SLURM<a class="headerlink" href="#running-jobs-with-slurm" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>SLURM is a management and job scheduling system at Linux clusters. More about <a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">SLURM quick references</a>.</p>
<p>Examples of slurm scripts are usually given at the program’s page with some recommendations for optimal use of resources for this particular program. List of the programs installed at HPC is given at our <a class="reference internal" href="software.html"><span class="doc">software page</span></a>.</p>
<div class="simple1">
The most often used SLURM commands are:<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">srun</span></code> - to start a session or an application (in real time)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sbatch</span></code> - to start a computation using a batch file (submit for later execution)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">squeue</span></code> - to check the load of the cluster and status of jobs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code> - to check the state of the cluster and partitions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scancel</span></code> - to delete a submitted job (or stop a running one).</p></li>
</ul>
</div>
<br><p>For more parameters see the man-pages (manual) of the commands <code class="docutils literal notranslate"><span class="pre">srun</span></code>, <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> and <code class="docutils literal notranslate"><span class="pre">squeue</span></code>. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>man<span class="w"> </span>srun
</pre></div>
</div>
<p>Requesting resources with SLURM can be done either with parameters to <code class="docutils literal notranslate"><span class="pre">srun</span></code> or in a batch script invoked by <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>. Unless otherwise specified, 1GB/thread will be used and the job will run for 10 minutes. <a class="reference external" href="/index.html#slurm-partitions">More about partitions and their limits</a>.</p>
<p><strong>Running an interactive session</strong> longer than default 10 min. (here 1 hour):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>-t<span class="w"> </span><span class="m">01</span>:00:00<span class="w"> </span>--pty<span class="w"> </span>bash<span class="w"> </span>
</pre></div>
</div>
<p>This logs you into one of the compute nodes, there you can load modules and run interactive applications, compile your code, etc.</p>
<p>With <code class="docutils literal notranslate"><span class="pre">srun</span></code> is reccomended to use CLI (command-line interface) instead of GUI (Graphical user interface) programs if it is possible. For example, use octave-CLI or octave instead of octave-GUI.</p>
<p><strong>Running a simple non-interactive single process job</strong> that lasts longer than default 4 hours (here 5 hours):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>--partition<span class="o">=</span>common<span class="w"> </span>-t<span class="w"> </span><span class="m">05</span>:00:00<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>./a.out
</pre></div>
</div>
<p><em><strong>NB!</strong></em> <em>Environment variables for OpenMP are <em>not</em> set automatically, e.g.</em></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w">  </span>-N<span class="w"> </span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">28</span><span class="w"> </span>./a.out
</pre></div>
</div>
<p>would <em>not</em> set <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> to 28, this has to be done manually. So usually, for parallel jobs it is recommended to use scripts for <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<p>Below is given <strong>an example of batch slurm script</strong> (filename: <code class="docutils literal notranslate"><span class="pre">myjob.slurm</span></code>) with explanation of the commands.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --partition=common    ### Partition</span>
<span class="c1">#SBATCH --job-name=HelloOMP   ### Job Name           -J</span>
<span class="c1">#SBATCH --time=00:10:00       ### Time limit         -t</span>
<span class="c1">#SBATCH --nodes=4             ### Number of Nodes    -N </span>
<span class="c1">#SBATCH --ntasks-per-node=7   ### Number of tasks (MPI processes)</span>
<span class="c1">#SBATCH --cpus-per-task=4     ### Number of threads per task (OMP threads)</span>
<span class="c1">#SBATCH --account=ptoject     ### Essentially your research group</span>
<span class="c1">#SBATCH --mem-per-cpu=100     ### Min RAM required in MB</span>
<span class="c1">#SBATCH --array=13-18         ### Array tasks for parameter sweep</span>
<span class="w">    </span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span><span class="w">    	</span><span class="c1">### setup environment</span>
<span class="w">    </span>module<span class="w"> </span>load<span class="w"> </span>gcc<span class="w">    					</span><span class="c1">### setup environment</span>
<span class="w">    </span>./hello_omp<span class="w"> </span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="w">    		</span><span class="c1">### only for arrays, setup output files with system information</span>

<span class="w">    </span>srun<span class="w"> </span>-n<span class="w"> </span><span class="m">28</span><span class="w"> </span>./hello_mpi<span class="w">     			</span><span class="c1">### run program</span>
</pre></div>
</div>
<p>In this example are listed some of the most common submission parameters. There are many other possible options, moreover, some of the options listed above  are not useful to apply together. Here can be found descriptions of the <a class="reference external" href="https://slurm.schedmd.com/sbatch.html#lbAJ">variables used inside SLURM/SBATCH</a> and <strong><a class="reference external" href="/slurm_example.html">more examples of SLURM scripts</a></strong> with more detailed explanations.</p>
<p>The job is then submitted to SLURM by</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>myjob.slurm
</pre></div>
</div>
<p>and will be executed when the requested resources become available.</p>
<p>Output of applications and error messages are by default written to a <code class="docutils literal notranslate"><span class="pre">slurm-$job_id.out</span></code> file. <a class="reference internal" href="slurm_statistics.html"><span class="doc">More about SLURM finished job statistics</span></a>.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="slurm-accounts">
<h2>SLURM accounts<a class="headerlink" href="#slurm-accounts" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>In SLURM exist accounts for billing, these are different from the login account!</p>
<p>Each user has his/her own personal SLURM-account, which will have a small monthly limit.  For larger calculations user should have at least one project account. SLURM user-accounts start with <code class="docutils literal notranslate"><span class="pre">user_</span></code> and project accounts with <code class="docutils literal notranslate"><span class="pre">project_</span></code> and course accounts with <code class="docutils literal notranslate"><span class="pre">course_</span></code>, followed by uniID/projectID/courseID.</p>
<p>You can check which SLURM accounts you belong to, by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sacctmgr<span class="w"> </span>show<span class="w"> </span>associations<span class="w"> </span><span class="nv">format</span><span class="o">=</span>account%30,user%30<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="nv">$USER</span>
</pre></div>
</div>
<p>Currently (almost) all users belong to the SLURM-account “vaikimisi” (default), it is possible to submit jobs under this account, especially if no <code class="docutils literal notranslate"><span class="pre">user_</span></code> or project account has been created for you yet, however, “vaikimisi” will be discontinued in the near future.</p>
<p>When submitting a job, <strong>it is important to use the correct SLURM-account</strong> <code class="docutils literal notranslate"><span class="pre">--account=SLURM-ACCOUNT</span></code>, as this is connected to the financial source.</p>
<br> 
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="monitoring-jobs-resources">
<h2>Monitoring jobs &amp; resources<a class="headerlink" href="#monitoring-jobs-resources" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<section id="monitoring-a-job-on-the-node">
<h3>Monitoring a job on the node<a class="headerlink" href="#monitoring-a-job-on-the-node" title="Permalink to this heading"></a></h3>
<section id="status-of-a-job">
<h4>Status of a job<a class="headerlink" href="#status-of-a-job" title="Permalink to this heading"></a></h4>
<p>User can check the status his jobs (whether they are running or not, and on which node) by the command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>squeue<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span>
</pre></div>
</div>
<p><img alt="squeue" src="_images/squeue.png" /></p>
</section>
<section id="load-of-the-node">
<h4>Load of the node<a class="headerlink" href="#load-of-the-node" title="Permalink to this heading"></a></h4>
<p>User can check the load of the node his job runs on, status and configuration of this node by command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>scontrol<span class="w"> </span>show<span class="w"> </span>node<span class="w"> </span>&lt;nodename&gt;
</pre></div>
</div>
<p>the load should not exceed the number of hyperthreads (CPUs in SLURM notation) of the node.</p>
<p><img alt="scontrol" src="_images/scontrol.png" /></p>
<p>In case of MPI parallel runs statistics of several nodes can be monitored by specifying nodes names. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>scontrol<span class="w"> </span>show<span class="w"> </span><span class="nv">node</span><span class="o">=</span>green<span class="o">[</span><span class="m">25</span>-26<span class="o">]</span>
</pre></div>
</div>
<p>Node features for node selection using <code class="docutils literal notranslate"><span class="pre">--constraint=</span></code>:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>feature</th>
<th>what it is</th>
</tr>
</thead>
<tbody>
<tr>
<td>A100-40</td>
<td>has A100 GPU with 40GB</td>
</tr>
<tr>
<td>A100-80</td>
<td>has A100 GPU with 80GB</td>
</tr>
<tr>
<td>L40</td>
<td>has L40 GPU with 48GB</td>
</tr>
<tr>
<td>nvcc80</td>
<td>GPU has compute capability 8.0 (A100, L40)</td>
</tr>
<tr>
<td>nvcc89</td>
<td>GPU has compute capability 8.9  (L40)</td>
</tr>
<tr>
<td>nvcc35</td>
<td>GPU has compute capability 3.5 (K20Xm, A100, L40)</td>
</tr>
<tr>
<td>zen2</td>
<td>AMD Zen CPU architecture 2nd generation (amp1)</td>
</tr>
<tr>
<td>zen3</td>
<td>AMD Zen CPU architecture 3rd generation (amp2)</td>
</tr>
<tr>
<td>zen4</td>
<td>AMD Zen CPU architecture 4th generation (ada*)</td>
</tr>
<tr>
<td>avx512</td>
<td>CPU has avx512 (skylake, zen4)</td>
</tr>
<tr>
<td>skylake</td>
<td>Intel SkyLake CPU architecture (green*)</td>
</tr>
<tr>
<td>sandybridge</td>
<td>Intel SandyBridge CPU architecture (mem1tb, viz)</td>
</tr>
<tr>
<td>ib</td>
<td>InfiniBand network interface</td>
</tr>
</tbody>
</table></section>
<section id="monitoring-with-interactive-job">
<h4>Monitoring with interactive job<a class="headerlink" href="#monitoring-with-interactive-job" title="Permalink to this heading"></a></h4>
<p>It is possible to submit a second interactive job to the node where the main job is running, check with <code class="docutils literal notranslate"><span class="pre">squeue</span></code> where your job is running, then submit</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun<span class="w"> </span>-w<span class="w"> </span>&lt;nodename&gt;<span class="w"> </span>--pty<span class="w"> </span>htop

Note<span class="w"> </span>that<span class="w"> </span>there<span class="w"> </span>must<span class="w"> </span>be<span class="w"> </span>free<span class="w"> </span>slots<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>machine,<span class="w"> </span>so<span class="w"> </span><span class="k">if</span><span class="w"> </span>you<span class="w"> </span>cannot<span class="w"> </span>use<span class="w"> </span><span class="sb">`</span>-n<span class="w"> </span><span class="m">80</span><span class="sb">`</span><span class="w"> </span>or<span class="w"> </span><span class="sb">`</span>--exclusive<span class="sb">`</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>your<span class="w"> </span>main<span class="w"> </span>job<span class="w"> </span><span class="o">(</span>use<span class="w"> </span><span class="sb">`</span>-n<span class="w"> </span><span class="m">79</span><span class="sb">`</span><span class="o">)</span>.

!<span class="o">[</span>htop<span class="o">](</span>pictures/htop.png<span class="o">)</span>

Press<span class="w"> </span><span class="sb">`</span>q<span class="sb">`</span><span class="w"> </span>to<span class="w"> </span>exit.

&lt;details&gt;&lt;summary&gt;You<span class="w"> </span>can<span class="w"> </span>also<span class="w"> </span>add<span class="w"> </span>a<span class="w"> </span>column<span class="w"> </span>that<span class="w"> </span>shows<span class="w"> </span>the<span class="w"> </span>CPU<span class="w"> </span>number<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>program<span class="w"> </span><span class="o">(</span><span class="k">for</span><span class="w"> </span>more<span class="w"> </span>details<span class="w"> </span>click<span class="w"> </span>here<span class="o">)</span>.&lt;/summary&gt;

For<span class="w"> </span>Linux<span class="w"> </span>**F1-F10**<span class="w"> </span>keys<span class="w"> </span>should<span class="w"> </span>be<span class="w"> </span>used,<span class="w"> </span><span class="k">for</span><span class="w"> </span>**Mac**<span class="w"> </span>-<span class="w"> </span>just<span class="w"> </span>click<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>corresponding<span class="w"> </span>buttons.

!<span class="o">[</span>htop-1<span class="o">](</span>pictures/htop-1.png<span class="o">)</span>

Will<span class="w"> </span>appear<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>column,<span class="w"> </span>showing<span class="w"> </span>the<span class="w"> </span>CPU<span class="w"> </span>number<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>program.

!<span class="o">[</span>htop-2<span class="o">](</span>pictures/htop-2.png<span class="o">)</span>

&lt;/details&gt;<span class="w">   </span>

&lt;br&gt;

<span class="c1">#### Monitoring jobs using GPUs</span>

Log<span class="w"> </span>to<span class="w"> </span>**amp**<span class="w"> </span>or<span class="w"> </span>**amp2**.<span class="w"> </span>Command<span class="w"> </span>
<span class="sb">```</span>bash
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">SLURM_STEP_GPUS</span><span class="k">:-</span><span class="nv">$SLURM_JOB_GPUS</span><span class="si">}</span><span class="w"> </span>
</pre></div>
</div>
<p>shows the GPU IDs allocated to your job.</p>
<p>GPUs load can be checked by command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>nvidia-smi
</pre></div>
</div>
<p><img alt="nvidia-smi" src="_images/nvidia-smi.png" /></p>
<p>Press <code class="docutils literal notranslate"><span class="pre">control+c</span></code> to exit.</p>
<p>Another option is to logging to <strong>amp</strong> or <strong>amp2</strong>, check which GPUs are allocated to your job, and give command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>nvtop
</pre></div>
</div>
<p><img alt="nvtop" src="_images/nvtop.png" /></p>
<p>Press <code class="docutils literal notranslate"><span class="pre">q</span></code> to exit.</p>
<p>An alternative method <strong>on Linux computers,</strong> if you have X11. Logging to <strong>base/amp</strong> with <code class="docutils literal notranslate"><span class="pre">--X</span></code> key:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>ssh<span class="w"> </span>--X<span class="w"> </span>UniID@base.hpc.taltech.ee
</pre></div>
</div>
<p>then submit your main interactive job</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>srun<span class="w"> </span>--x11<span class="w"> </span>-n<span class="w"> </span>&lt;numtasks&gt;<span class="w"> </span>--cpus-per-task<span class="o">=</span>&lt;numthreads&gt;<span class="w"> </span>--pty<span class="w"> </span>bash
</pre></div>
</div>
<p>and start an <code class="docutils literal notranslate"><span class="pre">xterm</span> <span class="pre">-e</span> <span class="pre">htop</span> <span class="pre">&amp;</span></code> in the session.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> the option <code class="docutils literal notranslate"><span class="pre">--x11=batch</span></code> can be used, note that the ssh session to <strong>base</strong> needs to stay open!</p>
</section>
</section>
<section id="monitoring-resource-usage">
<h3>Monitoring resource usage<a class="headerlink" href="#monitoring-resource-usage" title="Permalink to this heading"></a></h3>
<p>Default disc quota for <code class="docutils literal notranslate"><span class="pre">home</span></code> (that is backed up weekly) is 500 GB and for <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> (that is not backed up) – 2 TB per user. For <code class="docutils literal notranslate"><span class="pre">smbgroup</span></code> there is no limits and no backup.</p>
<p>The easiest way to check your current disk usage is to look at the table that appears when you log in to HPC.</p>
<p><img alt="disk_usage" src="_images/disk_usage.png" /></p>
<p>You can also monitor your resource usage by <code class="docutils literal notranslate"><span class="pre">taltech-lsquota.bash</span></code> script and <code class="docutils literal notranslate"><span class="pre">sreport</span></code> command.</p>
<p>Current disk usage:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>taltech-lsquota.bash
</pre></div>
</div>
<p><img alt="usage" src="_images/usage2.png" /></p>
<p>CPU usage during last day:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>sreport<span class="w"> </span>-t<span class="w"> </span>Hours<span class="w"> </span>cluster<span class="w"> </span>UserUtilizationByAccount<span class="w"> </span><span class="nv">Users</span><span class="o">=</span><span class="nv">$USER</span>
</pre></div>
</div>
<p><img alt="CPUhours" src="_images/CPUhours.png" /></p>
<p>CPU usage in specific period (e.g. since beginning of this year):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>sreport<span class="w"> </span>-t<span class="w"> </span>Hours<span class="w"> </span>cluster<span class="w"> </span>UserUtilizationByAccount<span class="w"> </span><span class="nv">Users</span><span class="o">=</span><span class="nv">$USER</span><span class="w"> </span><span class="nv">start</span><span class="o">=</span><span class="m">2024</span>-01-01T00:00:00<span class="w"> </span><span class="nv">end</span><span class="o">=</span><span class="m">2024</span>-12-31T23:59:59
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">start=</span></code> and <code class="docutils literal notranslate"><span class="pre">end=</span></code> can be changed depending on the desired period of time.</p>
<p><img alt="CPUhours_1" src="_images/CPUhours1.png" /></p>
<p>For convenience, a tool <code class="docutils literal notranslate"><span class="pre">taltech-history</span></code> was created, by default it shows the jobs of the current month, use <code class="docutils literal notranslate"><span class="pre">taltech-history</span> <span class="pre">-a</span></code> to get a summary of the useh hours and costs of the current month.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="copying-data-to-from-the-clusters">
<h2>Copying data to/from the clusters<a class="headerlink" href="#copying-data-to-from-the-clusters" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>Since HPC disk quota is limited, it is recommended to have your own copy of important calculations and results. Data from HPC can be transferred by several commands: <code class="docutils literal notranslate"><span class="pre">scp</span></code>, <code class="docutils literal notranslate"><span class="pre">sftp</span></code>, <code class="docutils literal notranslate"><span class="pre">sshfs</span></code> or <code class="docutils literal notranslate"><span class="pre">rsync</span></code>.</p>
<ol>
<li><p><code class="docutils literal notranslate"><span class="pre">scp</span></code> is available on all <strong>Linux systems,</strong> <strong>Mac</strong> and <strong>Windows10 PowerShell.</strong> There are also GUI versions available for different OS (like <a class="reference internal" href="putty.html"><span class="doc">PuTTY</span></a>).</p>
<p>Copying <em><strong>to</strong></em> the cluster with <code class="docutils literal notranslate"><span class="pre">scp</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">local_path_from_where_copy</span><span class="o">/</span><span class="n">file</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="p">:</span><span class="n">path_where_to_save</span>
</pre></div>
</div>
<p><img alt="scp" src="_images/scp1.png" /></p>
<p>Copying <em><strong>from</strong></em> the cluster with <code class="docutils literal notranslate"><span class="pre">scp</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="p">:</span><span class="n">path_from_where_copy</span><span class="o">/</span><span class="n">file</span> <span class="n">local_path_where_to_save</span> 
</pre></div>
</div>
<p><img alt="scp" src="_images/scp2.png" /></p>
<p>Path to the file at HPC can be checked by  <code class="docutils literal notranslate"><span class="pre">pwd</span></code> command.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">sftp</span></code> is the secure version of the <code class="docutils literal notranslate"><span class="pre">ftp</span></code> protocol vailable on <strong>Linux,</strong> <strong>Mac</strong> and <strong>Windows10 PowerShell.</strong> This command starts a session, in which files can be transmitted in both directions using the <code class="docutils literal notranslate"><span class="pre">get</span></code> and <code class="docutils literal notranslate"><span class="pre">put</span></code> commands. File transfer can be done in “binary” or “ascii” mode, conversion of line-endings (see below) is automatic in “ascii” mode. There are also GUI versions available for different OS (<a class="reference external" href="https://filezilla-project.org/">FileZilla</a>, <a class="reference external" href="https://github.com/masneyb/gftp">gFTP</a> and <a class="reference external" href="https://winscp.net/eng/index.php">WinSCP</a> (Windows))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sftp</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>
</pre></div>
</div>
<p><img alt="sftp" src="_images/sftp.png" /></p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">sshfs</span></code> can be used to temporarily mount remote filesystems for data transfer or analysis. Available in <strong>Linux.</strong> The data is tunneled through an ssh-connection. Be sware that this is usually not performant and can creates high load on the login node due to ssh-encryption.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sshfs</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="p">:</span><span class="n">remote_dir</span><span class="o">/</span> <span class="o">/</span><span class="n">path_to_local_mount_point</span><span class="o">/</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">rsync</span></code> can update files if previous versions exist without having to transfer the whole file. However, its use is recommended <strong>for the advanced user only</strong> since one has to be careful with the syntax.</p></li>
</ol>
<section id="smb-cifs-exported-filesystems">
<h3>SMB/CIFS exported filesystems<a class="headerlink" href="#smb-cifs-exported-filesystems" title="Permalink to this heading"></a></h3>
<p>One of the simple and convenient ways to control and process data based on HPC is mounting. Mounting means that user attaches his directory placed at HPC to a directory on his computer and can process files as if they were on this computer. These can be accessed from within university or from <a class="reference external" href="https://eduvpn.taltech.ee/vpn-user-portal/home">EduVPN</a>.</p>
<p>Each user automatically has a directory within <code class="docutils literal notranslate"><span class="pre">smbhome</span></code>. It does not match with <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> directory, so calculations should be initially done at <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> directory to prevent copying or files needed should be copied from <code class="docutils literal notranslate"><span class="pre">home</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> directory by commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="nb">pwd</span><span class="w">    </span><span class="c1">### look path to the file </span>
<span class="w">    </span>cp<span class="w"> </span>path_to_your_file/your_file<span class="w"> </span>/gpfs/mariana/smbhome/<span class="nv">$USER</span>/<span class="w">    </span><span class="c1">### copying</span>
</pre></div>
</div>
<p>To get a directory for group access, please contact us (a group and a directory need to be created).</p>
<p>The HPC center exports two filesystems as Windows network shares:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>local path on cluster</th>
<th>Linux network URL</th>
<th>Windows network URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>/gpfs/mariana/smbhome/$USER</td>
<td>smb://smb.hpc.taltech.ee/smbhome</td>
<td>\\smb.hpc.taltech.ee\smbhome</td>
</tr>
<tr>
<td>/gpfs/mariana/smbgroup</td>
<td>smb://smb.hpc.taltech.ee/smbgroup</td>
<td>\\smb.hpc.taltech.ee\smbgroup</td>
</tr>
<tr>
<td>/gpfs/mariana/home/$USER</td>
<td>not exported</td>
<td>not exported</td>
</tr>
</tbody>
</table><p><strong>This is the quick-access guide, for more details, see <a class="reference external" href="samba.html">here</a></strong></p>
<section id="windows-access">
<h4>Windows access<a class="headerlink" href="#windows-access" title="Permalink to this heading"></a></h4>
<p>The shares can be found using the Explorer “Map Network Drive”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="n">server</span> <span class="o">&gt;&gt;&gt;</span> \\<span class="n">smb</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>\<span class="n">smbhome</span>
    <span class="n">username</span> <span class="o">&gt;&gt;&gt;</span> <span class="n">INTRA</span>\<span class="o">&lt;</span><span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>From Powershell:</p>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="n">use</span> <span class="p">\\</span><span class="n">smb</span><span class="p">.</span><span class="n">hpc</span><span class="p">.</span><span class="n">taltech</span><span class="p">.</span><span class="n">ee</span><span class="p">\</span><span class="n">smbhome</span> <span class="p">/</span><span class="n">user</span><span class="p">:</span><span class="n">INTRA</span><span class="p">\</span><span class="n">uni-id</span>
<span class="nb">get-smbconnection</span>
</pre></div>
</div>
</section>
<section id="linux-access">
<h4>Linux access<a class="headerlink" href="#linux-access" title="Permalink to this heading"></a></h4>
<p>On Linux with GUI Desktop, the shares can be accessed with the nautilus browser.</p>
<p>From commandline, the shares can be mounted as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>dbus-run-session<span class="w"> </span>bash
<span class="w">    </span>gio<span class="w"> </span>mount<span class="w"> </span>smb://smb.hpc.taltech.ee/smbhome/
</pre></div>
</div>
<p>you will be asked for “User” (which is your UniID), “Domain” (which is “INTRA”), and your password.</p>
<p>To disconnect from the share, unmount with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">    </span>gio<span class="w"> </span>mount<span class="w"> </span>-u<span class="w"> </span>smb://smb.hpc.taltech.ee/smbhome/
</pre></div>
</div>
</section>
</section>
<section id="special-considerations-for-copying-windows-linux">
<h3>Special considerations for copying Windows - Linux<a class="headerlink" href="#special-considerations-for-copying-windows-linux" title="Permalink to this heading"></a></h3>
<p>Microsoft Windows is using a different line ending in text files (ASCII/UTF8 files) than Linux/Unix/Mac: CRLF vs. LF
When copying files between Windows-Linux, this needs to be taken into account. The FTP (File Transfer Protocol) has ASCII and BINARY modes, in ASCII-mode the line-end conversion is automatic.</p>
<p>There are tools for conversion of the line-ending, in case the file was copied without line conversion: <code class="docutils literal notranslate"><span class="pre">dos2unix</span></code>, <code class="docutils literal notranslate"><span class="pre">unix2dos</span></code>, <code class="docutils literal notranslate"><span class="pre">todos</span></code>, <code class="docutils literal notranslate"><span class="pre">fromdos</span></code>, the stream-editor <code class="docutils literal notranslate"><span class="pre">sed</span></code> can also be used.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="backup">
<h2>Backup<a class="headerlink" href="#backup" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>There are 2 major directories where users can store data:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/home/</span></code> default home directory which is limited to 500GB and is backed up, excluding specific directories: <code class="docutils literal notranslate"><span class="pre">[*/envs/,</span> <span class="pre">*/.cache/,</span> <span class="pre">*/pkgs/]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/smbhome/</span></code> has a limit of 2TB and is not backed up.</p></li>
</ul>
<p>The home directory is meant for critical data like configurations and scripts, whereas smbhome is meant for data.</p>
<p>The backup will run weekly. <em>If the home directory is larger than 500GB [usage is displayed upon login to the cluster] it will not be backed up.</em></p>
<p>If your home directory is larger than 500G please move the data to smbhome.</p>
<hr class="docutils" />
<p>At HPC are installed programs with varying licence agreement. To use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this contact us email (hpcsupport&#64;taltech.ee) or <a class="reference external" href="https://portal.taltech.ee/v2">Taltech portal</a>. More about available programs and licenses can be found at <a class="reference external" href="https://docs.hpc.taltech.ee/software.html">software page</a>.</p>
<br></section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cloud.html" class="btn btn-neutral float-left" title="Quickstart: Cloud" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="learning.html" class="btn btn-neutral float-right" title="Courses and introductions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright COPYRIGHT.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>