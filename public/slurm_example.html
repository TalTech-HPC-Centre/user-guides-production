<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Examples of slurm scripts &mdash; HPC user-guides 2024 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/extra.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #E4067E" >

          
          
          <a href="index.html" class="icon icon-home">
            HPC user-guides
              <img src="_static/TalTech_Gradient-200px.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="lumi.html">LUMI</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud.html">Quickstart: Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart: Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="learning.html">Courses and introductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Module environment (lmod)</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpi.html">Available MPI versions (and comparison)</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiling.html">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPU-servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity.html">Containers (Singularity &amp; Docker)</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgement.html">Acknowledgement</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #E4067E" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">HPC user-guides</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Examples of slurm scripts</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/slurm_example.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="examples-of-slurm-scripts">
<h1>Examples of slurm scripts<a class="headerlink" href="#examples-of-slurm-scripts" title="Permalink to this heading"></a></h1>
<div class="simple1">
<b>Some useful online resources:</b><ul class="simple">
<li><p><a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">SLURM scheduler workload manager</a></p></li>
<li><p>Victor Eijkhout: Introduction to High-Performance Scientific Computing <a href="https://doi.org/10.5281/zenodo.49897"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.49897.svg" alt="DOI"></a></p></li>
<li><p><a class="reference external" href="http://cnx.org/content/col11136/1.5/">Charles Severance, Kevin Dowd: High Performance Computing</a></p></li>
<li><p><a class="reference external" href="https://www.openmp.org/">OpenMP standard</a></p></li>
<li><p><a class="reference external" href="https://www.mpi-forum.org/">MPI standard</a></p></li>
<li><p><a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">SLURM Quick Reference (Cheat Sheet)</a></p></li>
</ul>
 </div> <br>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr><section id="a-single-thread-job">
<h2>A single thread job<a class="headerlink" href="#a-single-thread-job" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>The following script sets up the environment for ORCA and runs a job named ORCA_test using one thread and 2GB of memory, with job.inp as an input file. Calculations will be performed in a scratch directory on a node, not in your directory. Results will be written into job.log output file.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash </span>
<span class="c1">#SBATCH --job-name=ORCA_test		### job name </span>
<span class="c1">#SBATCH --ntasks=1			### number of threads   </span>
<span class="c1">#SBATCH --mem=2GB			### memory</span>
<span class="w">  </span>
<span class="c1">## load ORCA environment </span>
module<span class="w"> </span>load<span class="w"> </span>rocky8/all
module<span class="w"> </span>load<span class="w"> </span>orca/5.0.4
<span class="nb">export</span><span class="w"> </span><span class="nv">orcadir</span><span class="o">=</span>/gpfs/mariana/software/green/Orca/orca_5_0_4_openmpi_411/
<span class="w">   </span>
<span class="c1">#Create scratch directory</span>
<span class="nv">SCRATCH</span><span class="o">=</span>/state/partition1/<span class="nv">$SLURM_JOB_ID</span>
mkdir<span class="w"> </span>-p<span class="w"> </span><span class="nv">$SCRATCH</span>
cp<span class="w">  </span><span class="nv">$SLURM_SUBMIT_DIR</span>/*<span class="w"> </span><span class="nv">$SCRATCH</span>/
<span class="nb">cd</span><span class="w"> </span><span class="nv">$SCRATCH</span>/
<span class="w">  </span>
<span class="c1">#Run calculations </span>
<span class="nv">$orcadir</span>/orca<span class="w"> </span>job.inp<span class="w"> </span>&gt;&gt;<span class="w"> </span>job.log

<span class="c1">#Copy files back to working directory</span>
cp<span class="w"> </span><span class="nv">$SCRATCH</span>/*<span class="w"> </span><span class="nv">$SLURM_SUBMIT_DIR</span>
rm<span class="w"> </span>*tmp*

<span class="c1">#Clean after yourself</span>
rm<span class="w"> </span>-rf<span class="w">  </span><span class="nv">$SCRATCH</span>
</pre></div>
</div>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="an-openmp-parallel-job">
<h2>An OpenMP parallel job<a class="headerlink" href="#an-openmp-parallel-job" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>The following script launches job named HelloOMP using OpenMP. For this job slurm reserves one node and 12 threads. Maximum run time is 10 minutes.</p>
<span style="color:blue"> 
Even though it is `--cpus-per-task` slurm reserves threads, not CPU, since  "cpu" in SLURM's language is the smallest unit. </span> 
<br>
<br><p><strong>Note:</strong> Each thread needs to do enough work to compensate for the time it took to launch it. Therefore it is not useful to run small/short jobs in parallel.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=HelloOMP		### job name           -J</span>
<span class="c1">#SBATCH --time=00:10:00     		### time limit         -t</span>
<span class="c1">#SBATCH --nodes=1           		### number of nodes    -N </span>
<span class="c1">#SBATCH --ntasks-per-node=1 		### number of tasks (MPI processes)</span>
<span class="c1">#SBATCH --cpus-per-task=12  		### number of threads per task (OMP threads)</span>

<span class="c1">## load environment    </span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1">## run job</span>
./hello_omp<span class="w"> </span>
</pre></div>
</div>
<p><strong>NOTE: Parallel does not (necessarily) mean faster!!!</strong> Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="a-script-for-mpi-parallel-job-openfoam">
<h2>A script for MPI parallel job (OpenFOAM)<a class="headerlink" href="#a-script-for-mpi-parallel-job-openfoam" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>The following script reserves 4 CPU-cores for 10 hours
<span style="color:blue">
(since <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> uses cores by default),
</span><br />loads the OpenMPI module, the OpenFOAM variables, changes into the case directory and runs the typical commands necessary for a parallel OpenFOAM job. <em>It also sets OpenMPI transport properties to use Ethernet TCP!</em></p>
<p>It would be possible to request all tasks to be on the same node using the <code class="docutils literal notranslate"><span class="pre">-N</span></code> and <code class="docutils literal notranslate"><span class="pre">--tasks-per-node</span></code> options, this would be useful to make use of the very low latency shared memory communication of MPI (provided the job fits into the RAM of a single node).</p>
<span style="color:blue"> 
Flag `-l` in #!/bin/bash row means that settings in /home/user/.bash_profile will be executed.
</span>  
<br>
<br><p><strong>Note:</strong> Each task needs sufficient work to do to make up for the time spent with inter-process communication. Therefore it is not useful to run small/short jobs in parallel.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l    </span>
<span class="c1">#SBATCH -n 4     			### number of CPUs </span>
<span class="c1">#SBATCH -t 10:00:00      		### run time   </span>
<span class="c1">#SBATCH -J openfoam-damBreak     	### job name</span>
<span class="c1">#SBATCH --partition=green-ib		### partition </span>

<span class="c1">## load environment    </span>
module<span class="w"> </span>load<span class="w"> </span>rocky8-spack
module<span class="w"> </span>load<span class="w"> </span>openfoam

<span class="c1">## run program</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$WM_PROJECT_USER_DIR</span>/damBreak/damBreak
blockMesh
decomposePar
setFields
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="nv">$SLURM_NTASKS</span><span class="w"> </span>interFoam<span class="w"> </span>-parallel
reconstructPar
</pre></div>
</div>
<p><strong>NOTE: Parallel does not (necessarily) mean faster!!!</strong> Parallel execution introduces overhead (starting threads, communication)! For optimal execution time and optimal use of resources one needs to test and find the sweet spot.</p>
<p><img alt="sweet spot" src="_images/of-timing4.png" /></p>
<p>The division into the areas is a combined decision taking into account “real” (wall clock) and “user” (summed time of all threads) time (from the time command). “Wall clock” (real) time is the time one needs to wait till the job is finished, “Summed thread time” (user) is the sum of the times that all individual threads needed, it should be roughly user = numtreads x real. For parallel programs, one can expect that “user” time of the parallel run is larger than for the sequential, due to communication overhead, if it is smaller, that probably means the individual threads could make better use of cache.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>area</th>
<th>why</th>
<th>explanation</th>
<th>when to use</th>
</tr>
</thead>
<tbody>
<tr>
<td>sweet spot</td>
<td>minimal "user" time</td>
<td>= minimal heat production, optimal use of resources</td>
<td>regular use</td>
</tr>
<tr>
<td>good range</td>
<td>linear speedup for "real", with constant or slightly increasing "user"</td>
<td></td>
<td>approaching deadline</td>
</tr>
<tr>
<td>OK range</td>
<td>slightly less than linear speedup for "real", and slightly increasing "user"</td>
<td></td>
<td>pushing hard to make a deadline</td>
</tr>
<tr>
<td>avoid</td>
<td>ascending slope in the diagram for "real" and "user"</td>
<td>one actually needs to wait longer compared to the case with fewer cores</td>
<td>NEVER</td>
</tr>
</tbody>
</table><p>Recommended in <em>this</em> case would be to request 8 threads <code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">8</span> <span class="pre">--ntasks-per-node</span> <span class="pre">8</span></code> but use <code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">-n</span> <span class="pre">4</span></code>. OpenFOAM does not seem to benefit from hyperthreading
.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="an-array-parameter-sweep-job">
<h2>An array (parameter sweep) job<a class="headerlink" href="#an-array-parameter-sweep-job" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<span style="color:blue">
Tis script reserves 10 threads and run array of jobs in range of 13-1800.  The `$SLURM_ARRAY_TASK_ID` variable calls the input files in the given range in turn and data is written in output files arrayjob, which also contain job allocation ID and  job array index number (`-%A` and `-%a`, respectively). 
</span> 
<br>
<br><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash </span>
<span class="c1">#SBATCH --job-name=array-parameter-scan	### job name  </span>
<span class="c1">#SBATCH --output=arrayjob-%A-%a  		### output file </span>
<span class="c1">#SBATCH --ntasks=10  			    	### number of threads  </span>
<span class="c1">#SBATCH --array=13-1800       		    	### Array tasks for parameter sweep</span>
<span class="w">    </span>
<span class="c1">## run job</span>
./myarrayjob<span class="w">  </span><span class="nv">$SLURM_ARRAY_TASK_ID</span>
</pre></div>
</div>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="a-gpu-job">
<h2>A GPU job<a class="headerlink" href="#a-gpu-job" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>The GPU scripts can be run only on <strong>amp</strong>.</p>
<p>The following script reserves 1 gpu (Nvidia A100), uses gpu partition and has time limit 0f 10 minutes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash </span>
<span class="c1">#SBATCH --job-name=uses-gpu		### job name</span>
<span class="c1">#SBATCH -p gpu			### use gpu</span>
<span class="c1">#SBATCH --gres=gpu:A100:1		### specifying the GPU type</span>
<span class="c1">#SBATCH -t 00:10:00			### time limit </span>

<span class="c1">## run job    </span>
./mygpujob
</pre></div>
</div>
<p>This script reserves 4 gpu without specifying the GPU type.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash </span>
<span class="c1">#SBATCH --job-name=uses-gpu		### job name</span>
<span class="c1">#SBATCH -p gpu			### use gpu</span>
<span class="c1">#SBATCH --gres=gpu:4		### number of gpu</span>
<span class="c1">#SBATCH -t 00:10:00 		### time limit </span>

<span class="c1">## run job    </span>
./mygpujob
</pre></div>
</div>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="a-job-using-the-scratch-partition-sequential-or-openmp-parallel">
<h2>A job using the scratch partition (sequential or OpenMP parallel)<a class="headerlink" href="#a-job-using-the-scratch-partition-sequential-or-openmp-parallel" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<span style="color:blue"> 
The following script creates a directory named scratch-%x-%j (where -%x is a job name and %j is a jobid of the running job). This scratch directory is done on the scratch partition of the node to provide fast local storage, that does **not** require network. After, slurm script runs the job, and copies the output files back into the permanent home-directory once the job is completed.
</span>  
<br>
<br><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l    </span>
<span class="c1">#SBATCH -N 1			### number of nodes</span>
<span class="c1">#SBATCH -t 00:10:00			### time limit  </span>
<span class="c1">#SBATCH -J using-scratch		### job name</span>
<span class="w">    </span>
<span class="c1">## creates scratch scratch directory, copy files from working directory to scratch directory, goes to scratch directory</span>
mkdir<span class="w"> </span>/state/partition1/scratch-%x-%j
cp<span class="w"> </span>-r<span class="w"> </span><span class="nv">$HOME</span>/were/input/is/*<span class="w"> </span>/state/partition1/scratch-%x-%j/
<span class="nb">cd</span><span class="w"> </span>/state/partition1/scratch-%x-%j/

<span class="c1">## run job</span>
myjob

<span class="c1">## copy files from scratch directory to working directory and remove scratch directory</span>
cp<span class="w"> </span>-r<span class="w"> </span>/state/partition1/scratch-%x-%j/*<span class="w"> </span><span class="nv">$HOME</span>/were/input/is
rm<span class="w"> </span>-rf<span class="w"> </span>/state/partition1/scratch-%x-%j
</pre></div>
</div>
<p>Please note that the scratch is <em>not</em> shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other’s scratch files.</p>
<br></section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright COPYRIGHT.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>